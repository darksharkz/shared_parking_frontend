\chapter{Security Analysis}
\label{ch:Security Analysis}
In the following, the design of the system is evaluated using the security features set up in the requirement analysis. We look at each point individually and use the premises set up in section x to prove that we meet the requirements. To show that the reputations module is resistant to the attacks mentioned in section x, we will use the evaluation of DTSRS presented in \cite{mousa2017reputation}.

\section{Fraud Detection}
Fraud detection includes two components: the simple recognition of the fact of fraud as well as the ability to correctly identify the adversary.  \\

\subparagraph{Fraud recognition} Fraud recognition is exclusively achieved by the report system. As soon as a user detects a rule violation, he can report it and the system is informed. It is sufficient to recognize fraud by which another party would be harmed, because this would not be any different without the Shared Parking System. A parking offence only causes problems if another user, who is authorized to park, wants to park on it and with our system the other user now has the ability to report it. Thus, parking spaces with a shared parking system always provide a better possibility for fraud detection than without it. \\

\subparagraph{Adversary identification}The ability to correctly identify the adversary is provided by the verification and reputation module. \\

First, we show that our reputation module can generally distinguish between malicious and genuine users, while also defending itself against the attacks described by class 2 fraud cases.

Our module is based on the DTSRS from \cite{mousa2017reputation}, in which Mousa et. al experimentally evaluated their system using different simulations and obtained the following results. They showed that their system "accurately assesses the quality of participants' contributions" and "clearly identifies adversaries even if the number of colluding adversaries reaches 60\% of the total number of participants" and furthermore defends itself against corruption, On-Off and collusion attacks. \\

Since we only map our system parameters to the parameters of Participatory Sensing, we can adopt the evaluation and the resulting properties of DTSRS. We assume that the majority of users in our system are non-malicious (\ref{}), which means an adversary or a collaboration of adversaries can control at maximum 50\% of users. This remains below the 60\% with which DTSRS can still successfully deal with. In the following we discuss how the $SPS$ can either fend off attacks on a reputation system presented in the adversary model or mitigate them in the environment of our system in some other way.

\begin{enumerate}
\item \textbf{Corruption attack:} DTSRS was shown to be resiliant to the corruption attack, and our system inherits this property from DTSRS.
\item \textbf{On-off attack:} DTSRS was shown to be resiliant to the on-off attack, and our system inherits this property from DTSRS.
\item \textbf{Re-entry attack:} As Mousa et. al. states in \cite{mousa2015trust} "this attack was not considered by any of the current reputation based trust systems in participatory sensing" and thus no automated solution has been found in state-of-the-art reputation systems. If we want to protect our system against this type of fraud, we could add a manual function: the registration credential information would have to be checked manually either in general during registration  (for example, with proof of identity card) or during registration of a user's license plate (in cooperation with the authorities). This ensures that people can only register with their real identity and thus only once.
\item \textbf{Collusion attack:} DTSRS was shown to be resiliant to the Collusion attack, and our system inherits this property from DTSRS.
\item \textbf{Sybil attack:} For the sybil attack the same applies as for the re-entry attack.
\item \textbf{Reputation lag exploitation:} In our system there is a time interval between the individual events of a user which cannot be influenced by the user himself. For malicous behavior as inspector, the user must first be selected as such. If parking spaces are blocked, the system is only informed when the user is reported, etc. It is not possible for a user to launch a large number of attacks in a very short time. Thus, the results of a fraud are already accounted for in the reputation score of the user when the next fraud case is evaluated.
\item \textbf{GPS spoofing attacks:} We do not use gps functions when receiving contributions because we already know the position of an event. If an inspector checks a fraud case, it is a parking space that is not available. The system already knows the position of this parking space. If an inspector does not travel to the location of the incident and simply guesses a result of his verification, it is called a corruption attack.
\item \textbf{Unfair ratings:} DTSRS implemented methods to mitigate the unfair rating attack, and our system inherits this property from DTSRS.
\item \textbf{Bath mouthing or negative discrimination attack:} "Bad mouthing [...] attacks are not addressed by any of existing trust systems in participatory sensing"  explained in \cite{mousa2015trust}. However, the effect of this attack is also mitigated in our system. In contrast to DTSRS and Participatory Sensing, users cannot arbitrarily give feedback in our system. To rate another user, you must first book or rent a parking space. While renting out one cannot freely choose the target of his bad mouthing attack. While renting one has to pay the parking price for every single rating he'd like to issue. In addition, the ratings of each individual user are publicly available and other users can easily recognize if the adversary discriminates against individual users. In addition, users who are unfairly rated badly will also return a bad rating in most cases.
\item \textbf{Ballot stuffing or positive discrimination attack:} Ballot stuffing is similar to bad mouthing. Even if two users always rate each other highly, fees must be paid to the operator for each related rental transaction.
\end{enumerate}

Because the reputation module can correctly distinguish between malicious and genuine users and it is used to check each report, we can decide whether the reporting or the reported user is the adversary. The trust of a contribution used for this is actually better than simply comparing the trust scores of the conflicting users, since in addition to the trust score, the contributions of the other users for this event and the proximity factor are also included there. \\

As shown, the module recognizes the adversaries of class 1 cases (reported user is a adversary) and recognizes and protects itself against class 2 cases. The exceptional cases of class 3 are briefly discussed below: 


\section{Fraud Punishment}
The ability of fraud punishment is based on the ability of fraud detection. If a adversary is correctly identified, the system is able to penalize him. By registering, users have agreed to the rules of the system and can therefore be fined if they violate those rules. In the simplest case, the system receives the money by deducting it from the offender's credit balance and adding it to the system's credit balance. It is therefore obvious that the system receives the entire penalty and can use it for other purposes. If the adversary's credit balance becomes negative through deduction and is not recharged within a certain period of time, the operator receives the penalty in the form of an invoice for contractual penalty to the user.

\section{Fraud Prevention}
The system achieves fraud prevention through two methods. On the one hand, adversaries are deterred and on the other hand, fraud is uncovered and adversaries are thus eliminated.

adversaries are deterred by the public rating system and the well-known possibility of reporting. Anyone who intends to cheat considers twice whether they want to accept the resulting disadvantages, such as a poor rating or exclusion from the system. In addition, there is the fine you have to pay if you are discovered to be a adversary. Since these fines are invariably higher than the cost of using the system properly, fraud is not profitable.

As explained in the paragraph 'Fraud Dedection', adversaries are discovered at the latest in the long run. On one hand, all other users notice from the adversary's rating that there is something wrong. The adversary is thus eliminated by the fact that nobody wants to book a parking space of his any more and he will no longer be able to book a parking space from users with manual confirmation, because most users only conclude contracts with users from whom they assume that everything will go as planned, i.e. users with a good rating. On the other hand, adversaries are eliminated because they are excluded by the system. Anyone who has been discovered too often as a adversary or whose reputation score falls below a certain threshold will be banned.

\section{Fraud Compensation}
Also, fraud compensation can easily be performed once the system has the ability of correctly identifying the fraudster. Further up we have shown that the system has this ability. Once the fraudster has been clearly identified, it is also clear that the other party to the conflict is the injured party which must be compensated. \\

But even before this is clear, the reporting user was provided with a new parking space if required. The costs are initially borne by the system. However, once fraudsters and injured parties have been assigned, the system will recover these costs, as shown in the 'Fraud Punishment' section. This means that there are no costs for the system if the adversary can be clearly identified. So if the reporting user was really the injured party, he got a replacement parking space without paying anything for it; he was compensated. If the reporting user was the adversary, however, he had to pay a fine afterwards, meaning that he paid for his replacement parking space by himself.